# -*- coding: utf-8 -*-
"""word_embedding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a_2OKVZl-rldBSA9qzO3uonJcGeZCJOj
"""

from tensorflow import keras
import tensorflow as tf
import pandas as pd
import numpy as np

data = pd.read_csv("lemmatized_training.csv")

texts = data["text"]
labels = data["label"]

tokenizer = tf.keras.preprocessing.text.Tokenizer()
tokenizer.fit_on_texts(texts)

vocab_len = len(tokenizer.word_index)
sequences = tokenizer.texts_to_sequences(texts)
max_len = 60
for i in range(len(sequences)): sequences[i] += [0]*(60-len(sequences[i]))
train = np.array([np.array(e) for e in sequences])

model = keras.Sequential([
                          keras.layers.Embedding(vocab_len + 1, 48, mask_zero=True),
                          keras.layers.GlobalAveragePooling1D(),
                          keras.layers.Dense(units = 16, activation = "relu"),
                          keras.layers.Dense(units = 6, activation = "softmax"),
])
model.compile(optimizer=keras.optimizers.RMSprop(), loss="sparse_categorical_crossentropy", metrics=["accuracy"])
model.summary()

model.fit(train[1000:], labels[1000:], validation_data=(train[:1000],labels[:1000]),epochs = 1, batch_size=64)

test_df = pd.read_csv("lemmatized_test.csv")["text"]
test_sequences = tokenizer.texts_to_sequences(test_df)
max_len = 60
for i in range(len(test_sequences)): test_sequences[i] += [0]*(60-len(test_sequences[i]))
test = np.array([np.array(e) for e in test_sequences])
prediction = model.predict(test)

submission = pd.read_csv("sample.csv")
for i in range(len(prediction)): 
  submission["label"][i] = list(prediction[i]).index(max(prediction[i]))
submission.to_csv("submission.csv", index=False)